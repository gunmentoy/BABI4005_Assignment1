{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c4664b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load all CSV files from the NBA_data folder\n",
    "filepath = 'NBA_data/'\n",
    "csv_files = glob.glob(os.path.join(filepath, '*.csv'))\n",
    "\n",
    "# Create a dictionary to store all dataframes\n",
    "dfs = {}\n",
    "for file in sorted(csv_files):\n",
    "    filename = os.path.basename(file)\n",
    "    dfs[filename] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c74d5",
   "metadata": {},
   "source": [
    "## Data Completness Analysis\n",
    "\n",
    "This section provides a comprehensive overview of data quality across all NBA datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9990c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA COMPLETENESS ANALYSIS\n",
      "\n",
      "common_player_info.csv\n",
      "Rows: 4171 | Columns: 33\n",
      "Completeness: 96.74%\n",
      "Missing Data in 15 columns:\n",
      "    - school: 15 (0.4%)\n",
      "    - country: 1 (0.0%)\n",
      "    - height: 96 (2.3%)\n",
      "    - weight: 100 (2.4%)\n",
      "    - jersey: 980 (23.5%)\n",
      "    - position: 63 (1.5%)\n",
      "    - team_name: 702 (16.8%)\n",
      "    - team_abbreviation: 702 (16.8%)\n",
      "    - team_code: 702 (16.8%)\n",
      "    - team_city: 702 (16.8%)\n",
      "    - playercode: 1 (0.0%)\n",
      "    - from_year: 15 (0.4%)\n",
      "    - to_year: 15 (0.4%)\n",
      "    - draft_round: 164 (3.9%)\n",
      "    - draft_number: 223 (5.3%)\n",
      "\n",
      "game.csv\n",
      "Rows: 65698 | Columns: 55\n",
      "Completeness: 88.02%\n",
      "Missing Data in 36 columns:\n",
      "    - wl_home: 2 (0.0%)\n",
      "    - fgm_home: 13 (0.0%)\n",
      "    - fga_home: 15447 (23.5%)\n",
      "    - fg_pct_home: 15490 (23.6%)\n",
      "    - fg3m_home: 13218 (20.1%)\n",
      "    - fg3a_home: 18683 (28.4%)\n",
      "    - fg3_pct_home: 19074 (29.0%)\n",
      "    - ftm_home: 16 (0.0%)\n",
      "    - fta_home: 3004 (4.6%)\n",
      "    - ft_pct_home: 3009 (4.6%)\n",
      "    - oreb_home: 18936 (28.8%)\n",
      "    - dreb_home: 18999 (28.9%)\n",
      "    - reb_home: 15729 (23.9%)\n",
      "    - ast_home: 15805 (24.1%)\n",
      "    - stl_home: 18849 (28.7%)\n",
      "    - blk_home: 18626 (28.4%)\n",
      "    - tov_home: 18684 (28.4%)\n",
      "    - pf_home: 2856 (4.3%)\n",
      "    - wl_away: 2 (0.0%)\n",
      "    - fgm_away: 13 (0.0%)\n",
      "    - fga_away: 15447 (23.5%)\n",
      "    - fg_pct_away: 15489 (23.6%)\n",
      "    - fg3m_away: 13218 (20.1%)\n",
      "    - fg3a_away: 18683 (28.4%)\n",
      "    - fg3_pct_away: 18962 (28.9%)\n",
      "    - ftm_away: 13 (0.0%)\n",
      "    - fta_away: 3004 (4.6%)\n",
      "    - ft_pct_away: 3006 (4.6%)\n",
      "    - oreb_away: 18936 (28.8%)\n",
      "    - dreb_away: 18998 (28.9%)\n",
      "    - reb_away: 15725 (23.9%)\n",
      "    - ast_away: 15801 (24.1%)\n",
      "    - stl_away: 18849 (28.7%)\n",
      "    - blk_away: 18625 (28.3%)\n",
      "    - tov_away: 18685 (28.4%)\n",
      "    - pf_away: 2851 (4.3%)\n",
      "\n",
      "game_summary.csv\n",
      "Rows: 58110 | Columns: 14\n",
      "Completeness: 80.39%\n",
      "Missing Data in 4 columns:\n",
      "    - game_sequence: 25532 (43.9%)\n",
      "    - game_status_text: 25986 (44.7%)\n",
      "    - live_pc_time: 56086 (96.5%)\n",
      "    - natl_tv_broadcaster_abbreviation: 51907 (89.3%)\n",
      "\n",
      "inactive_players.csv\n",
      "Rows: 110191 | Columns: 9\n",
      "Completeness: 100.00%\n",
      "Missing Data in 3 columns:\n",
      "    - first_name: 1 (0.0%)\n",
      "    - last_name: 1 (0.0%)\n",
      "    - jersey_num: 43 (0.0%)\n",
      "\n",
      "line_score.csv\n",
      "Rows: 58053 | Columns: 43\n",
      "Completeness: 68.14%\n",
      "Missing Data in 29 columns:\n",
      "    - game_sequence: 25532 (44.0%)\n",
      "    - pts_qtr1_home: 1004 (1.7%)\n",
      "    - pts_qtr2_home: 1013 (1.7%)\n",
      "    - pts_qtr3_home: 1045 (1.8%)\n",
      "    - pts_qtr4_home: 1044 (1.8%)\n",
      "    - pts_ot1_home: 25759 (44.4%)\n",
      "    - pts_ot2_home: 27051 (46.6%)\n",
      "    - pts_ot3_home: 27243 (46.9%)\n",
      "    - pts_ot4_home: 27270 (47.0%)\n",
      "    - pts_ot5_home: 45577 (78.5%)\n",
      "    - pts_ot6_home: 45578 (78.5%)\n",
      "    - pts_ot7_home: 45578 (78.5%)\n",
      "    - pts_ot8_home: 45578 (78.5%)\n",
      "    - pts_ot9_home: 45578 (78.5%)\n",
      "    - pts_ot10_home: 45578 (78.5%)\n",
      "    - pts_qtr1_away: 1010 (1.7%)\n",
      "    - pts_qtr2_away: 1013 (1.7%)\n",
      "    - pts_qtr3_away: 1046 (1.8%)\n",
      "    - pts_qtr4_away: 1046 (1.8%)\n",
      "    - pts_ot1_away: 25759 (44.4%)\n",
      "    - pts_ot2_away: 27051 (46.6%)\n",
      "    - pts_ot3_away: 27243 (46.9%)\n",
      "    - pts_ot4_away: 27270 (47.0%)\n",
      "    - pts_ot5_away: 45577 (78.5%)\n",
      "    - pts_ot6_away: 45578 (78.5%)\n",
      "    - pts_ot7_away: 45578 (78.5%)\n",
      "    - pts_ot8_away: 45578 (78.5%)\n",
      "    - pts_ot9_away: 45578 (78.5%)\n",
      "    - pts_ot10_away: 45578 (78.5%)\n",
      "\n",
      "other_stats.csv\n",
      "Rows: 28271 | Columns: 26\n",
      "Completeness: 98.79%\n",
      "Missing Data in 8 columns:\n",
      "    - team_turnovers_home: 2 (0.0%)\n",
      "    - total_turnovers_home: 316 (1.1%)\n",
      "    - team_rebounds_home: 1998 (7.1%)\n",
      "    - pts_off_to_home: 2123 (7.5%)\n",
      "    - team_turnovers_away: 2 (0.0%)\n",
      "    - total_turnovers_away: 316 (1.1%)\n",
      "    - team_rebounds_away: 1998 (7.1%)\n",
      "    - pts_off_to_away: 2123 (7.5%)\n",
      "\n",
      "play_by_play.csv\n",
      "Rows: 13592899 | Columns: 34\n",
      "Completeness: 63.84%\n",
      "Rows: 13592899 | Columns: 34\n",
      "Completeness: 63.84%\n",
      "Missing Data in 22 columns:\n",
      "    - wctimestring: 950 (0.0%)\n",
      "    - homedescription: 6530241 (48.0%)\n",
      "    - neutraldescription: 13273327 (97.6%)\n",
      "    - visitordescription: 6634527 (48.8%)\n",
      "    - score: 10028436 (73.8%)\n",
      "    - scoremargin: 10028436 (73.8%)\n",
      "    - person1type: 3298 (0.0%)\n",
      "    - player1_name: 1208875 (8.9%)\n",
      "    - player1_team_id: 1215858 (8.9%)\n",
      "    - player1_team_city: 1215858 (8.9%)\n",
      "    - player1_team_nickname: 1215858 (8.9%)\n",
      "    - player1_team_abbreviation: 1215858 (8.9%)\n",
      "    - player2_name: 9683745 (71.2%)\n",
      "    - player2_team_id: 9660454 (71.1%)\n",
      "    - player2_team_city: 9660454 (71.1%)\n",
      "    - player2_team_nickname: 9660454 (71.1%)\n",
      "    - player2_team_abbreviation: 9660454 (71.1%)\n",
      "    - player3_name: 13251785 (97.5%)\n",
      "    - player3_team_id: 13246931 (97.5%)\n",
      "    - player3_team_city: 13246931 (97.5%)\n",
      "    - player3_team_nickname: 13246931 (97.5%)\n",
      "    - player3_team_abbreviation: 13246931 (97.5%)\n",
      "\n",
      "player.csv\n",
      "Rows: 4831 | Columns: 5\n",
      "Completeness: 99.98%\n",
      "Missing Data in 1 columns:\n",
      "    - first_name: 6 (0.1%)\n",
      "\n",
      "team.csv\n",
      "Rows: 30 | Columns: 7\n",
      "Completeness: 100.00%\n",
      "Missing Data in 22 columns:\n",
      "    - wctimestring: 950 (0.0%)\n",
      "    - homedescription: 6530241 (48.0%)\n",
      "    - neutraldescription: 13273327 (97.6%)\n",
      "    - visitordescription: 6634527 (48.8%)\n",
      "    - score: 10028436 (73.8%)\n",
      "    - scoremargin: 10028436 (73.8%)\n",
      "    - person1type: 3298 (0.0%)\n",
      "    - player1_name: 1208875 (8.9%)\n",
      "    - player1_team_id: 1215858 (8.9%)\n",
      "    - player1_team_city: 1215858 (8.9%)\n",
      "    - player1_team_nickname: 1215858 (8.9%)\n",
      "    - player1_team_abbreviation: 1215858 (8.9%)\n",
      "    - player2_name: 9683745 (71.2%)\n",
      "    - player2_team_id: 9660454 (71.1%)\n",
      "    - player2_team_city: 9660454 (71.1%)\n",
      "    - player2_team_nickname: 9660454 (71.1%)\n",
      "    - player2_team_abbreviation: 9660454 (71.1%)\n",
      "    - player3_name: 13251785 (97.5%)\n",
      "    - player3_team_id: 13246931 (97.5%)\n",
      "    - player3_team_city: 13246931 (97.5%)\n",
      "    - player3_team_nickname: 13246931 (97.5%)\n",
      "    - player3_team_abbreviation: 13246931 (97.5%)\n",
      "\n",
      "player.csv\n",
      "Rows: 4831 | Columns: 5\n",
      "Completeness: 99.98%\n",
      "Missing Data in 1 columns:\n",
      "    - first_name: 6 (0.1%)\n",
      "\n",
      "team.csv\n",
      "Rows: 30 | Columns: 7\n",
      "Completeness: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Data Completeness Analysis\n",
    "# This shows how much data is present or missing in each dataset\n",
    "print(\"DATA COMPLETENESS ANALYSIS\")\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    print(f\"\\n{filename}\")\n",
    "    total_rows = len(df)\n",
    "    total_cells = total_rows * len(df.columns)\n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    completeness = ((total_cells - missing_cells) / total_cells * 100)\n",
    "    \n",
    "    print(f\"Rows: {total_rows} | Columns: {len(df.columns)}\")\n",
    "    print(f\"Completeness: {completeness:.2f}%\")\n",
    "    \n",
    "    # Show columns with missing data\n",
    "    missing_cols = df.isnull().sum()\n",
    "    missing_cols = missing_cols[missing_cols > 0]\n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"Missing Data in {len(missing_cols)} columns:\")\n",
    "        for col, count in missing_cols.items():\n",
    "            pct = (count / total_rows * 100)\n",
    "            print(f\"    - {col}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27e8e2",
   "metadata": {},
   "source": [
    "## Data Type Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c7d4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA TYPE ANALYSIS\n",
      "\n",
      "common_player_info.csv\n",
      "Data Types:\n",
      "  - object: 27 columns\n",
      "  - float64: 4 columns\n",
      "  - int64: 2 columns\n",
      "\n",
      "game.csv\n",
      "Data Types:\n",
      "  - float64: 36 columns\n",
      "  - object: 10 columns\n",
      "  - int64: 9 columns\n",
      "\n",
      "game_summary.csv\n",
      "Data Types:\n",
      "  - int64: 7 columns\n",
      "  - object: 6 columns\n",
      "  - float64: 1 columns\n",
      "\n",
      "inactive_players.csv\n",
      "Data Types:\n",
      "  - object: 5 columns\n",
      "  - int64: 3 columns\n",
      "  - float64: 1 columns\n",
      "\n",
      "line_score.csv\n",
      "Data Types:\n",
      "  - float64: 31 columns\n",
      "  - object: 9 columns\n",
      "  - int64: 3 columns\n",
      "\n",
      "other_stats.csv\n",
      "Data Types:\n",
      "  - int64: 14 columns\n",
      "  - float64: 8 columns\n",
      "  - object: 4 columns\n",
      "\n",
      "play_by_play.csv\n",
      "Data Types:\n",
      "  - object: 19 columns\n",
      "  - int64: 9 columns\n",
      "  - float64: 6 columns\n",
      "\n",
      "player.csv\n",
      "Data Types:\n",
      "  - object: 3 columns\n",
      "  - int64: 2 columns\n",
      "\n",
      "team.csv\n",
      "Data Types:\n",
      "  - object: 5 columns\n",
      "  - int64: 1 columns\n",
      "  - float64: 1 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA TYPE ANALYSIS\")\n",
    "\n",
    "# For each dataset, show the data types\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    print(f\"\\n{filename}\")\n",
    "    print(\"Data Types:\")\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  - {dtype}: {count} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a920e8",
   "metadata": {},
   "source": [
    "## Data Validity Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be58622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA VALIDITY ANALYSIS - Check for Duplicates and Anomalies\n",
      "\n",
      "common_player_info.csv\n",
      "✓ No duplicate rows found\n",
      "Numeric columns: 6\n",
      "• person_id: min=2.00, max=1631347.00, mean=332750.86\n",
      "• weight: min=133.00, max=360.00, mean=211.13\n",
      "• season_exp: min=0.00, max=22.00, mean=5.20\n",
      "\n",
      "game.csv\n",
      "✓ No duplicate rows found\n",
      "Numeric columns: 45\n",
      "• season_id: min=12005.00, max=42022.00, mean=22949.34\n",
      "• team_id_home: min=45.00, max=1610616834.00, mean=1609926285.57\n",
      "• game_id: min=10500001.00, max=49800087.00, mean=25847473.13\n",
      "\n",
      "game_summary.csv\n",
      "⚠ Duplicate rows: 40 (0.07%)\n",
      "Numeric columns: 8\n",
      "• game_sequence: min=0.00, max=15.00, mean=4.25\n",
      "• game_id: min=10500001.00, max=49800087.00, mean=25804934.26\n",
      "• game_status_id: min=1.00, max=3.00, mean=3.00\n",
      "\n",
      "inactive_players.csv\n",
      "⚠ Duplicate rows: 7 (0.01%)\n",
      "Numeric columns: 4\n",
      "• game_id: min=10500008.00, max=42200405.00, mean=22363519.44\n",
      "• player_id: min=0.00, max=1962935994.00, mean=664491.96\n",
      "• jersey_num: min=0.00, max=99.00, mean=18.66\n",
      "\n",
      "line_score.csv\n",
      "⚠ Duplicate rows: 40 (0.07%)\n",
      "Numeric columns: 34\n",
      "• game_sequence: min=0.00, max=15.00, mean=4.25\n",
      "• game_id: min=10500001.00, max=49800087.00, mean=25797904.00\n",
      "• team_id_home: min=41.00, max=1610616834.00, mean=1609502967.65\n",
      "\n",
      "other_stats.csv\n",
      "⚠ Duplicate rows: 10 (0.04%)\n",
      "Numeric columns: 22\n",
      "• game_id: min=11000001.00, max=49800086.00, mean=23144513.49\n",
      "• league_id: min=0.00, max=0.00, mean=0.00\n",
      "• team_id_home: min=41.00, max=1610616834.00, mean=1608618800.52\n",
      "\n",
      "play_by_play.csv\n",
      "⚠ Duplicate rows: 40 (0.07%)\n",
      "Numeric columns: 34\n",
      "• game_sequence: min=0.00, max=15.00, mean=4.25\n",
      "• game_id: min=10500001.00, max=49800087.00, mean=25797904.00\n",
      "• team_id_home: min=41.00, max=1610616834.00, mean=1609502967.65\n",
      "\n",
      "other_stats.csv\n",
      "⚠ Duplicate rows: 10 (0.04%)\n",
      "Numeric columns: 22\n",
      "• game_id: min=11000001.00, max=49800086.00, mean=23144513.49\n",
      "• league_id: min=0.00, max=0.00, mean=0.00\n",
      "• team_id_home: min=41.00, max=1610616834.00, mean=1608618800.52\n",
      "\n",
      "play_by_play.csv\n",
      "⚠ Duplicate rows: 7360 (0.05%)\n",
      "⚠ Duplicate rows: 7360 (0.05%)\n",
      "Numeric columns: 15\n",
      "• game_id: min=11300001.00, max=49800087.00, mean=23241016.75\n",
      "• eventnum: min=0.00, max=1018.00, mean=272.53\n",
      "• eventmsgtype: min=1.00, max=18.00, mean=3.98\n",
      "\n",
      "player.csv\n",
      "✓ No duplicate rows found\n",
      "Numeric columns: 2\n",
      "• id: min=2.00, max=1631466.00, mean=333205.05\n",
      "• is_active: min=0.00, max=1.00, mean=0.12\n",
      "\n",
      "team.csv\n",
      "✓ No duplicate rows found\n",
      "Numeric columns: 2\n",
      "• id: min=1610612737.00, max=1610612766.00, mean=1610612751.50\n",
      "• year_founded: min=1946.00, max=2002.00, mean=1969.70\n",
      "Numeric columns: 15\n",
      "• game_id: min=11300001.00, max=49800087.00, mean=23241016.75\n",
      "• eventnum: min=0.00, max=1018.00, mean=272.53\n",
      "• eventmsgtype: min=1.00, max=18.00, mean=3.98\n",
      "\n",
      "player.csv\n",
      "✓ No duplicate rows found\n",
      "Numeric columns: 2\n",
      "• id: min=2.00, max=1631466.00, mean=333205.05\n",
      "• is_active: min=0.00, max=1.00, mean=0.12\n",
      "\n",
      "team.csv\n",
      "✓ No duplicate rows found\n",
      "Numeric columns: 2\n",
      "• id: min=1610612737.00, max=1610612766.00, mean=1610612751.50\n",
      "• year_founded: min=1946.00, max=2002.00, mean=1969.70\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA VALIDITY ANALYSIS - Check for Duplicates and Anomalies\")\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    print(f\"\\n{filename}\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"⚠ Duplicate rows: {duplicates} ({(duplicates/len(df))*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"✓ No duplicate rows found\")\n",
    "    \n",
    "    # Check for potential issues in numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "        for col in numeric_cols[:3]:  # Show first 3 as sample\n",
    "            print(f\"• {col}: min={df[col].min():.2f}, max={df[col].max():.2f}, mean={df[col].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b4b13",
   "metadata": {},
   "source": [
    "## Data Accuracy Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16405786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA ACCURACY ANALYSIS - Check for Outliers and Ranges\n",
      "\n",
      "common_player_info.csv\n",
      "⚠ Found 1686 potential outliers across numeric columns\n",
      "\n",
      "game.csv\n",
      "⚠ Found 72509 potential outliers across numeric columns\n",
      "\n",
      "game_summary.csv\n",
      "⚠ Found 9316 potential outliers across numeric columns\n",
      "\n",
      "inactive_players.csv\n",
      "⚠ Found 8388 potential outliers across numeric columns\n",
      "\n",
      "line_score.csv\n",
      "⚠ Found 72509 potential outliers across numeric columns\n",
      "\n",
      "game_summary.csv\n",
      "⚠ Found 9316 potential outliers across numeric columns\n",
      "\n",
      "inactive_players.csv\n",
      "⚠ Found 8388 potential outliers across numeric columns\n",
      "\n",
      "line_score.csv\n",
      "⚠ Found 18042 potential outliers across numeric columns\n",
      "\n",
      "other_stats.csv\n",
      "⚠ Found 13131 potential outliers across numeric columns\n",
      "\n",
      "play_by_play.csv\n",
      "⚠ Found 18042 potential outliers across numeric columns\n",
      "\n",
      "other_stats.csv\n",
      "⚠ Found 13131 potential outliers across numeric columns\n",
      "\n",
      "play_by_play.csv\n",
      "⚠ Found 12946410 potential outliers across numeric columns\n",
      "\n",
      "player.csv\n",
      "⚠ Found 1385 potential outliers across numeric columns\n",
      "\n",
      "team.csv\n",
      "✓ No significant outliers detected\n",
      "⚠ Found 12946410 potential outliers across numeric columns\n",
      "\n",
      "player.csv\n",
      "⚠ Found 1385 potential outliers across numeric columns\n",
      "\n",
      "team.csv\n",
      "✓ No significant outliers detected\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA ACCURACY ANALYSIS - Check for Outliers and Ranges\")\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    print(f\"\\n{filename}\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # Check for outliers using IQR method\n",
    "        outlier_count = 0\n",
    "        for col in numeric_cols:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outlier_count += len(outliers)\n",
    "        \n",
    "        if outlier_count > 0:\n",
    "            print(f\"⚠ Found {outlier_count} potential outliers across numeric columns\")\n",
    "        else:\n",
    "            print(f\"✓ No significant outliers detected\")\n",
    "    else:\n",
    "        print(f\"No numeric columns to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52c426fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions for Accuracy Analysis\n",
    "\n",
    "# 1. Range Validation - check if values fall within expected ranges\n",
    "def validate_range(df, col, min_val=None, max_val=None, allow_null=True):\n",
    "    \"\"\"\n",
    "    Check if values in a column fall within acceptable ranges.\n",
    "    Returns: dict with counts of violations\n",
    "    \"\"\"\n",
    "    s = df[col].copy()\n",
    "    violations = {'below_min': 0, 'above_max': 0, 'total_checked': 0}\n",
    "    \n",
    "    if allow_null:\n",
    "        s = s.dropna()\n",
    "    \n",
    "    violations['total_checked'] = len(s)\n",
    "    \n",
    "    if min_val is not None:\n",
    "        violations['below_min'] = int((s < min_val).sum())\n",
    "    if max_val is not None:\n",
    "        violations['above_max'] = int((s > max_val).sum())\n",
    "    \n",
    "    return violations\n",
    "\n",
    "# 2. Referential Integrity Check - verify IDs exist across datasets\n",
    "def check_referential_integrity(source_df, source_id_col, target_dfs_dict):\n",
    "    \"\"\"\n",
    "    Check if IDs in source_df exist in target dataframes.\n",
    "    Returns: dict with missing ID counts per target\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    source_ids = set(source_df[source_id_col].dropna().unique())\n",
    "    \n",
    "    for target_name, target_df in target_dfs_dict.items():\n",
    "        # Try to find matching ID column\n",
    "        id_cols = [c for c in target_df.columns if source_id_col in c.lower()]\n",
    "        if id_cols:\n",
    "            target_ids = set(target_df[id_cols[0]].dropna().unique())\n",
    "            missing = source_ids - target_ids\n",
    "            results[target_name] = {\n",
    "                'missing_ids': len(missing),\n",
    "                'total_source_ids': len(source_ids),\n",
    "                'pct_missing': 100.0 * len(missing) / len(source_ids) if source_ids else 0.0\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 3. Plausibility Checks - domain-specific NBA data validation\n",
    "def nba_plausibility_checks(df):\n",
    "    \"\"\"\n",
    "    Run NBA-specific plausibility checks on stats columns.\n",
    "    Returns: dict with flags and violation counts\n",
    "    \"\"\"\n",
    "    checks = {}\n",
    "    \n",
    "    # Minutes played: should be 0-60 (48min regulation + 12min OT typical)\n",
    "    min_cols = [c for c in df.columns if 'min' in c.lower() and df[c].dtype in ['int64', 'float64']]\n",
    "    for col in min_cols:\n",
    "        violations = (df[col] > 60).sum()\n",
    "        checks[f'{col}_exceeds_60'] = {'violations': int(violations), 'ratio': violations / len(df) if len(df) else 0}\n",
    "    \n",
    "    # Shooting percentages: should be 0-100\n",
    "    pct_cols = [c for c in df.columns if 'pct' in c.lower() and df[c].dtype in ['int64', 'float64']]\n",
    "    for col in pct_cols:\n",
    "        below = (df[col] < 0).sum()\n",
    "        above = (df[col] > 100).sum()\n",
    "        checks[f'{col}_pct_invalid'] = {'violations': int(below + above), 'ratio': (below + above) / len(df) if len(df) else 0}\n",
    "    \n",
    "    # Points, rebounds, assists, etc: should be >= 0\n",
    "    stat_cols = [c for c in df.columns if any(x in c.lower() for x in ['pts', 'reb', 'ast', 'stl', 'blk', 'tov', 'pf'])]\n",
    "    for col in stat_cols:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            violations = (df[col] < 0).sum()\n",
    "            checks[f'{col}_negative'] = {'violations': int(violations), 'ratio': violations / len(df) if len(df) else 0}\n",
    "    \n",
    "    return checks\n",
    "\n",
    "# 4. Cross-Dataset Reconciliation - compare aggregated stats\n",
    "def reconcile_stats(play_df, game_df, stat_col, groupby_cols, tolerance=0):\n",
    "    \"\"\"\n",
    "    Aggregate stats from play_df and compare with game_df.\n",
    "    Returns: DataFrame with mismatches flagged\n",
    "    \"\"\"\n",
    "    if stat_col not in play_df.columns or stat_col not in game_df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Aggregate play-by-play data\n",
    "    agg_play = play_df.groupby(groupby_cols)[stat_col].sum().reset_index()\n",
    "    agg_play.rename(columns={stat_col: f'{stat_col}_play'}, inplace=True)\n",
    "    \n",
    "    # Merge with game data\n",
    "    merged = agg_play.merge(game_df[groupby_cols + [stat_col]], how='outer', on=groupby_cols, suffixes=('_play', '_game'))\n",
    "    merged.rename(columns={stat_col: f'{stat_col}_game'}, inplace=True)\n",
    "    \n",
    "    # Calculate difference\n",
    "    merged[f'{stat_col}_diff'] = merged[f'{stat_col}_game'].fillna(0) - merged[f'{stat_col}_play'].fillna(0)\n",
    "    merged[f'{stat_col}_absdiff'] = merged[f'{stat_col}_diff'].abs()\n",
    "    merged[f'{stat_col}_flag'] = merged[f'{stat_col}_absdiff'] > tolerance\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# 5. Sampling for Manual Review\n",
    "def sample_for_manual_review(df, by_cols=None, n_per_group=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Sample representative records for manual verification.\n",
    "    Returns: sampled DataFrame\n",
    "    \"\"\"\n",
    "    if by_cols and all(c in df.columns for c in by_cols):\n",
    "        return df.groupby(by_cols, group_keys=False).apply(\n",
    "            lambda x: x.sample(min(len(x), n_per_group), random_state=random_state)\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        return df.sample(min(len(df), n_per_group * 5), random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "print(\"✓ Helper functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a745647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "EXPANDED ACCURACY ANALYSIS - Plausibility & Internal Consistency Checks\n",
      "====================================================================================================\n",
      "\n",
      "common_player_info\n",
      "--------------------------------------------------------------------------------\n",
      "  Range Validation:\n",
      "  Referential Integrity:\n",
      "    ⚠ team_id: 4125 duplicate values (98.90%)\n",
      "\n",
      "game\n",
      "--------------------------------------------------------------------------------\n",
      "  Plausibility Checks:\n",
      "    ⚠ min_exceeds_60: 60085 violations (91.46%)\n",
      "    ⚠ plus_minus_home_exceeds_60: 7 violations (0.01%)\n",
      "    ⚠ plus_minus_away_exceeds_60: 1 violations (0.00%)\n",
      "  Range Validation:\n",
      "    ⚠ min: 60085 violations (91.46%)\n",
      "    ⚠ plus_minus_home: 25053 violations (38.13%)\n",
      "    ⚠ plus_minus_away: 40653 violations (61.88%)\n",
      "    ✓ Range checks: 91.3% of checked values passed\n",
      "  Referential Integrity:\n",
      "    ⚠ season_id: 65473 duplicate values (99.66%)\n",
      "    ⚠ team_id_home: 65635 duplicate values (99.90%)\n",
      "    ⚠ game_id: 56 duplicate values (0.09%)\n",
      "    ⚠ video_available_home: 65696 duplicate values (100.00%)\n",
      "    ⚠ team_id_away: 65626 duplicate values (99.89%)\n",
      "    ⚠ video_available_away: 65696 duplicate values (100.00%)\n",
      "\n",
      "game_summary\n",
      "--------------------------------------------------------------------------------\n",
      "  Range Validation:\n",
      "  Referential Integrity:\n",
      "    ⚠ game_id: 89 duplicate values (0.15%)\n",
      "    ⚠ game_status_id: 58108 duplicate values (100.00%)\n",
      "    ⚠ home_team_id: 58047 duplicate values (99.89%)\n",
      "    ⚠ visitor_team_id: 58039 duplicate values (99.88%)\n",
      "\n",
      "inactive_players\n",
      "--------------------------------------------------------------------------------\n",
      "  Range Validation:\n",
      "  Referential Integrity:\n",
      "    ⚠ game_id: 89879 duplicate values (81.57%)\n",
      "    ⚠ player_id: 108101 duplicate values (98.10%)\n",
      "    ⚠ team_id: 110155 duplicate values (99.97%)\n",
      "\n",
      "line_score\n",
      "--------------------------------------------------------------------------------\n",
      "  Plausibility Checks:\n",
      "    ✓ All plausibility checks passed\n",
      "  Range Validation:\n",
      "    ✓ Range checks: 100.0% of checked values passed\n",
      "  Referential Integrity:\n",
      "    ⚠ game_id: 40 duplicate values (0.07%)\n",
      "    ⚠ team_id_home: 57986 duplicate values (99.88%)\n",
      "    ⚠ team_id_away: 57978 duplicate values (99.87%)\n",
      "\n",
      "other_stats\n",
      "--------------------------------------------------------------------------------\n",
      "  Plausibility Checks:\n",
      "    ✓ All plausibility checks passed\n",
      "  Range Validation:\n",
      "    ✓ Range checks: 100.0% of checked values passed\n",
      "  Referential Integrity:\n",
      "    ⚠ game_id: 10 duplicate values (0.04%)\n",
      "    ⚠ league_id: 28270 duplicate values (100.00%)\n",
      "    ⚠ team_id_home: 28222 duplicate values (99.83%)\n",
      "    ⚠ team_id_away: 28226 duplicate values (99.84%)\n",
      "\n",
      "play_by_play\n",
      "--------------------------------------------------------------------------------\n",
      "  Range Validation:\n",
      "  Referential Integrity:\n",
      "    ⚠ game_id: 13563081 duplicate values (99.78%)\n",
      "    ⚠ player1_id: 13589041 duplicate values (99.97%)\n",
      "    ⚠ player1_team_id: 1215858 null values (8.94%)\n",
      "  Referential Integrity:\n",
      "    ⚠ game_id: 13563081 duplicate values (99.78%)\n",
      "    ⚠ player1_id: 13589041 duplicate values (99.97%)\n",
      "    ⚠ player1_team_id: 1215858 null values (8.94%)\n",
      "    ⚠ player1_team_id: 13592847 duplicate values (100.00%)\n",
      "    ⚠ player2_id: 13589786 duplicate values (99.98%)\n",
      "    ⚠ player2_team_id: 9660454 null values (71.07%)\n",
      "    ⚠ player2_team_id: 13592847 duplicate values (100.00%)\n",
      "    ⚠ player1_team_id: 13592847 duplicate values (100.00%)\n",
      "    ⚠ player2_id: 13589786 duplicate values (99.98%)\n",
      "    ⚠ player2_team_id: 9660454 null values (71.07%)\n",
      "    ⚠ player2_team_id: 13592847 duplicate values (100.00%)\n",
      "    ⚠ player3_id: 13590420 duplicate values (99.98%)\n",
      "    ⚠ player3_team_id: 13246931 null values (97.45%)\n",
      "    ⚠ player3_team_id: 13592849 duplicate values (100.00%)\n",
      "    ⚠ video_available_flag: 13592897 duplicate values (100.00%)\n",
      "\n",
      "player\n",
      "--------------------------------------------------------------------------------\n",
      "  Range Validation:\n",
      "  Referential Integrity:\n",
      "    ✓ All ID columns have integrity\n",
      "\n",
      "team\n",
      "--------------------------------------------------------------------------------\n",
      "  Range Validation:\n",
      "  Referential Integrity:\n",
      "    ✓ All ID columns have integrity\n",
      "\n",
      "====================================================================================================\n",
      "    ⚠ player3_id: 13590420 duplicate values (99.98%)\n",
      "    ⚠ player3_team_id: 13246931 null values (97.45%)\n",
      "    ⚠ player3_team_id: 13592849 duplicate values (100.00%)\n",
      "    ⚠ video_available_flag: 13592897 duplicate values (100.00%)\n",
      "\n",
      "player\n",
      "--------------------------------------------------------------------------------\n",
      "  Range Validation:\n",
      "  Referential Integrity:\n",
      "    ✓ All ID columns have integrity\n",
      "\n",
      "team\n",
      "--------------------------------------------------------------------------------\n",
      "  Range Validation:\n",
      "  Referential Integrity:\n",
      "    ✓ All ID columns have integrity\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Expanded Accuracy Analysis - Plausibility & Cross-Dataset Checks\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EXPANDED ACCURACY ANALYSIS - Plausibility & Internal Consistency Checks\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "accuracy_results = []\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    dataset_name = filename.replace('.csv', '')\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = {\n",
    "        'Dataset': dataset_name,\n",
    "        'Plausibility_Checks': [],\n",
    "        'Range_Violations': [],\n",
    "        'Referential_Issues': []\n",
    "    }\n",
    "    \n",
    "    # ===== PLAUSIBILITY CHECKS =====\n",
    "    plausibility = nba_plausibility_checks(df)\n",
    "    if plausibility:\n",
    "        print(f\"  Plausibility Checks:\")\n",
    "        total_violations = 0\n",
    "        for check_name, check_result in plausibility.items():\n",
    "            violations = check_result['violations']\n",
    "            ratio = check_result['ratio']\n",
    "            if violations > 0:\n",
    "                print(f\"    ⚠ {check_name}: {violations} violations ({100*ratio:.2f}%)\")\n",
    "                total_violations += violations\n",
    "            result['Plausibility_Checks'].append(f\"{check_name}={violations}\")\n",
    "        if total_violations == 0:\n",
    "            print(f\"    ✓ All plausibility checks passed\")\n",
    "    \n",
    "    # ===== RANGE CHECKS (Domain-Specific) =====\n",
    "    print(f\"  Range Validation:\")\n",
    "    range_checks_config = {\n",
    "        'MIN': (0, 60),  # minutes\n",
    "        'FG_PCT': (0, 100),  # field goal %\n",
    "        'FT_PCT': (0, 100),  # free throw %\n",
    "        'FG3_PCT': (0, 100),  # 3-point %\n",
    "    }\n",
    "    \n",
    "    range_pass_count = 0\n",
    "    range_total_count = 0\n",
    "    \n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        col_lower = col.lower()\n",
    "        min_val, max_val = None, None\n",
    "        \n",
    "        # Auto-detect range based on column name\n",
    "        if 'pct' in col_lower or 'percentage' in col_lower:\n",
    "            min_val, max_val = 0, 100\n",
    "        elif any(x in col_lower for x in ['min', 'minute']):\n",
    "            min_val, max_val = 0, 60\n",
    "        elif any(x in col_lower for x in ['pts', 'reb', 'ast', 'stl', 'blk', 'tov', 'pf']):\n",
    "            min_val, max_val = 0, None\n",
    "        \n",
    "        if min_val is not None or max_val is not None:\n",
    "            violations = validate_range(df, col, min_val, max_val)\n",
    "            total_violations = violations['below_min'] + violations['above_max']\n",
    "            \n",
    "            if total_violations > 0:\n",
    "                pct = 100.0 * total_violations / violations['total_checked'] if violations['total_checked'] else 0\n",
    "                print(f\"    ⚠ {col}: {total_violations} violations ({pct:.2f}%)\")\n",
    "                result['Range_Violations'].append(f\"{col}={total_violations}\")\n",
    "            \n",
    "            range_pass_count += violations['total_checked'] - total_violations\n",
    "            range_total_count += violations['total_checked']\n",
    "    \n",
    "    if range_total_count > 0:\n",
    "        range_pass_rate = 100.0 * range_pass_count / range_total_count\n",
    "        print(f\"    ✓ Range checks: {range_pass_rate:.1f}% of checked values passed\")\n",
    "    \n",
    "    # ===== REFERENTIAL INTEGRITY =====\n",
    "    ref_issues = False\n",
    "    id_cols = [c for c in df.columns if 'id' in c.lower()]\n",
    "    if id_cols:\n",
    "        print(f\"  Referential Integrity:\")\n",
    "        for id_col in id_cols:\n",
    "            # Check for null IDs\n",
    "            null_ids = df[id_col].isnull().sum()\n",
    "            if null_ids > 0:\n",
    "                print(f\"    ⚠ {id_col}: {null_ids} null values ({100*null_ids/len(df):.2f}%)\")\n",
    "                result['Referential_Issues'].append(f\"{id_col}_nulls={null_ids}\")\n",
    "                ref_issues = True\n",
    "            \n",
    "            # Check for duplicates/uniqueness\n",
    "            dup_ids = df[id_col].duplicated().sum()\n",
    "            if dup_ids > 0:\n",
    "                print(f\"    ⚠ {id_col}: {dup_ids} duplicate values ({100*dup_ids/len(df):.2f}%)\")\n",
    "                result['Referential_Issues'].append(f\"{id_col}_dups={dup_ids}\")\n",
    "                ref_issues = True\n",
    "        \n",
    "        if not ref_issues:\n",
    "            print(f\"    ✓ All ID columns have integrity\")\n",
    "    \n",
    "    accuracy_results.append(result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c7d78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "SAMPLING & SUSPICIOUS RECORDS - For Manual Verification\n",
      "====================================================================================================\n",
      "\n",
      "common_player_info\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 4171\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "game\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 65698\n",
      "  Suspicious records: 60085 (91.46%)\n",
      "  Sample of 10 suspicious records (for manual review):\n",
      "    • Row 43198: min_exceeds_60\n",
      "    • Row 17618: min_exceeds_60\n",
      "    • Row 44623: min_exceeds_60\n",
      "    • Row 38201: min_exceeds_60\n",
      "    • Row 40626: min_exceeds_60\n",
      "    • Row 9029: min_exceeds_60\n",
      "    • Row 64676: min_exceeds_60\n",
      "    • Row 42392: min_exceeds_60\n",
      "    • Row 26703: min_exceeds_60\n",
      "    • Row 12543: min_exceeds_60\n",
      "\n",
      "game_summary\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 58110\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "inactive_players\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 110191\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "line_score\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 58053\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "other_stats\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 28271\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "play_by_play\n",
      "--------------------------------------------------------------------------------\n",
      "  Sample of 10 suspicious records (for manual review):\n",
      "    • Row 43198: min_exceeds_60\n",
      "    • Row 17618: min_exceeds_60\n",
      "    • Row 44623: min_exceeds_60\n",
      "    • Row 38201: min_exceeds_60\n",
      "    • Row 40626: min_exceeds_60\n",
      "    • Row 9029: min_exceeds_60\n",
      "    • Row 64676: min_exceeds_60\n",
      "    • Row 42392: min_exceeds_60\n",
      "    • Row 26703: min_exceeds_60\n",
      "    • Row 12543: min_exceeds_60\n",
      "\n",
      "game_summary\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 58110\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "inactive_players\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 110191\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "line_score\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 58053\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "other_stats\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 28271\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "play_by_play\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 13592899\n",
      "  Suspicious records: 13540639 (99.62%)\n",
      "  Total records: 13592899\n",
      "  Suspicious records: 13540639 (99.62%)\n",
      "  Sample of 10 suspicious records (for manual review):\n",
      "    • Row 7268014: null_player3_team_id\n",
      "    • Row 3971385: null_player2_team_id;null_player3_team_id\n",
      "    • Row 10470346: null_player3_team_id\n",
      "    • Row 3722600: null_player2_team_id;null_player3_team_id\n",
      "    • Row 8172904: null_player3_team_id\n",
      "    • Row 6308358: null_player2_team_id;null_player3_team_id\n",
      "    • Row 2108499: null_player2_team_id;null_player3_team_id\n",
      "    • Row 8420397: null_player3_team_id\n",
      "    • Row 4825251: null_player3_team_id\n",
      "    • Row 9428124: null_player1_team_id;null_player2_team_id;null_player3_team_id\n",
      "\n",
      "player\n",
      "--------------------------------------------------------------------------------\n",
      "  Sample of 10 suspicious records (for manual review):\n",
      "    • Row 7268014: null_player3_team_id\n",
      "    • Row 3971385: null_player2_team_id;null_player3_team_id\n",
      "    • Row 10470346: null_player3_team_id\n",
      "    • Row 3722600: null_player2_team_id;null_player3_team_id\n",
      "    • Row 8172904: null_player3_team_id\n",
      "    • Row 6308358: null_player2_team_id;null_player3_team_id\n",
      "    • Row 2108499: null_player2_team_id;null_player3_team_id\n",
      "    • Row 8420397: null_player3_team_id\n",
      "    • Row 4825251: null_player3_team_id\n",
      "    • Row 9428124: null_player1_team_id;null_player2_team_id;null_player3_team_id\n",
      "\n",
      "player\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 4831\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "team\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 30\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "✓ Suspicious records exported to: suspicious_records_for_review.csv\n",
      "====================================================================================================\n",
      "  Total records: 4831\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "team\n",
      "--------------------------------------------------------------------------------\n",
      "  Total records: 30\n",
      "  Suspicious records: 0 (0.00%)\n",
      "  ✓ No suspicious records detected\n",
      "\n",
      "✓ Suspicious records exported to: suspicious_records_for_review.csv\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sampling & Suspicious Records Flagging for Manual Verification\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SAMPLING & SUSPICIOUS RECORDS - For Manual Verification\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "suspicious_records_log = []\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    dataset_name = filename.replace('.csv', '')\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create a suspicious flag column\n",
    "    df_copy = df.copy()\n",
    "    df_copy['_suspicious'] = False\n",
    "    df_copy['_flags'] = ''\n",
    "    \n",
    "    # Flag records with plausibility issues\n",
    "    plausibility = nba_plausibility_checks(df_copy)\n",
    "    for check_name, check_result in plausibility.items():\n",
    "        violations = check_result['violations']\n",
    "        if violations > 0:\n",
    "            if 'pct_invalid' in check_name:\n",
    "                col = check_name.replace('_pct_invalid', '')\n",
    "                mask = (df_copy[col] < 0) | (df_copy[col] > 100)\n",
    "                df_copy.loc[mask, '_suspicious'] = True\n",
    "                df_copy.loc[mask, '_flags'] += f'{check_name};'\n",
    "            elif 'exceeds_60' in check_name:\n",
    "                col = check_name.replace('_exceeds_60', '')\n",
    "                mask = (df_copy[col] > 60)\n",
    "                df_copy.loc[mask, '_suspicious'] = True\n",
    "                df_copy.loc[mask, '_flags'] += f'{check_name};'\n",
    "            elif 'negative' in check_name:\n",
    "                col = check_name.replace('_negative', '')\n",
    "                mask = (df_copy[col] < 0)\n",
    "                df_copy.loc[mask, '_suspicious'] = True\n",
    "                df_copy.loc[mask, '_flags'] += f'{check_name};'\n",
    "    \n",
    "    # Flag records with missing IDs\n",
    "    id_cols = [c for c in df_copy.columns if 'id' in c.lower()]\n",
    "    for id_col in id_cols:\n",
    "        mask = df_copy[id_col].isnull()\n",
    "        df_copy.loc[mask, '_suspicious'] = True\n",
    "        df_copy.loc[mask, '_flags'] += f'null_{id_col};'\n",
    "    \n",
    "    # Count and sample\n",
    "    suspicious_count = df_copy['_suspicious'].sum()\n",
    "    total_count = len(df_copy)\n",
    "    \n",
    "    print(f\"  Total records: {total_count}\")\n",
    "    print(f\"  Suspicious records: {suspicious_count} ({100*suspicious_count/total_count if total_count else 0:.2f}%)\")\n",
    "    \n",
    "    if suspicious_count > 0:\n",
    "        # Sample suspicious records for review\n",
    "        suspicious_df = df_copy[df_copy['_suspicious']].copy()\n",
    "        sample_size = min(10, len(suspicious_df))\n",
    "        sample = suspicious_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        print(f\"  Sample of {sample_size} suspicious records (for manual review):\")\n",
    "        \n",
    "        # Log sample flags\n",
    "        for idx, row in sample.iterrows():\n",
    "            flags = row['_flags'].rstrip(';')\n",
    "            print(f\"    • Row {idx}: {flags}\")\n",
    "            suspicious_records_log.append({\n",
    "                'dataset': dataset_name,\n",
    "                'row_index': int(idx),\n",
    "                'flags': flags,\n",
    "                'record': str(row.drop(['_suspicious', '_flags']).to_dict())[:100]\n",
    "            })\n",
    "    else:\n",
    "        print(f\"  ✓ No suspicious records detected\")\n",
    "\n",
    "# Export suspicious records to CSV for review\n",
    "if suspicious_records_log:\n",
    "    suspicious_df = pd.DataFrame(suspicious_records_log)\n",
    "    suspicious_df.to_csv('suspicious_records_for_review.csv', index=False)\n",
    "    print(f\"\\n✓ Suspicious records exported to: suspicious_records_for_review.csv\")\n",
    "else:\n",
    "    print(f\"\\n✓ No suspicious records to export\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c55c4",
   "metadata": {},
   "source": [
    "## Data Timeliness Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cce13f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA TIMELINESS ANALYSIS - Check for Date/Time Columns\n",
      "\n",
      "common_player_info.csv\n",
      "No datetime columns detected in this file\n",
      "\n",
      "game.csv\n",
      "No datetime columns detected in this file\n",
      "\n",
      "game_summary.csv\n",
      "No datetime columns detected in this file\n",
      "\n",
      "inactive_players.csv\n",
      "No datetime columns detected in this file\n",
      "\n",
      "line_score.csv\n",
      "No datetime columns detected in this file\n",
      "\n",
      "other_stats.csv\n",
      "No datetime columns detected in this file\n",
      "\n",
      "play_by_play.csv\n",
      "No datetime columns detected in this file\n",
      "\n",
      "player.csv\n",
      "No datetime columns detected in this file\n",
      "\n",
      "team.csv\n",
      "No datetime columns detected in this file\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA TIMELINESS ANALYSIS - Check for Date/Time Columns\")\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    print(f\"\\n{filename}\")\n",
    "    \n",
    "    # Check for datetime columns\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns\n",
    "    \n",
    "    if len(datetime_cols) > 0:\n",
    "        print(f\"Found {len(datetime_cols)} datetime column(s):\")\n",
    "        for col in datetime_cols:\n",
    "            min_date = df[col].min()\n",
    "            max_date = df[col].max()\n",
    "            print(f\"    • {col}: {min_date} to {max_date}\")\n",
    "    else:\n",
    "        print(f\"No datetime columns detected in this file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac9aa8",
   "metadata": {},
   "source": [
    "## Data Consistency Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9617e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CONSISTENCY ANALYSIS - Check for Uniform Data Formats\n",
      "\n",
      " common_player_info.csv\n",
      "Categorical columns: 27\n",
      "• first_name: 1434 unique values\n",
      "• last_name: 2576 unique values\n",
      "• display_first_last: 4139 unique values\n",
      "\n",
      " game.csv\n",
      "Categorical columns: 10\n",
      "• team_abbreviation_home: 97 unique values\n",
      "• team_name_home: 98 unique values\n",
      "• game_date: 12882 unique values\n",
      "\n",
      " game_summary.csv\n",
      "Categorical columns: 6\n",
      "• game_date_est: 12610 unique values\n",
      "• game_status_text: 5 unique values\n",
      "• gamecode: 58021 unique values\n",
      "\n",
      " inactive_players.csv\n",
      "Categorical columns: 5\n",
      "• first_name: 1099 unique values\n",
      "• last_name: 1426 unique values\n",
      "• team_city: 41 unique values\n",
      "\n",
      " line_score.csv\n",
      "Categorical columns: 9\n",
      "• game_date_est: 12610 unique values\n",
      "• team_abbreviation_home: 103 unique values\n",
      "• team_city_name_home: 76 unique values\n",
      "\n",
      " other_stats.csv\n",
      "Categorical columns: 4\n",
      "• team_abbreviation_home: 58 unique values\n",
      "• team_city_home: 54 unique values\n",
      "• team_abbreviation_away: 56 unique values\n",
      "\n",
      " play_by_play.csv\n",
      "Categorical columns: 19\n",
      "Categorical columns: 19\n",
      "• wctimestring: 4128 unique values\n",
      "• pctimestring: 721 unique values\n",
      "• wctimestring: 4128 unique values\n",
      "• pctimestring: 721 unique values\n",
      "• homedescription: 1837935 unique values\n",
      "\n",
      " player.csv\n",
      "Categorical columns: 3\n",
      "• full_name: 4791 unique values\n",
      "• first_name: 1568 unique values\n",
      "• last_name: 2904 unique values\n",
      "\n",
      " team.csv\n",
      "Categorical columns: 5\n",
      "• full_name: 30 unique values\n",
      "• abbreviation: 30 unique values\n",
      "• nickname: 30 unique values\n",
      "• homedescription: 1837935 unique values\n",
      "\n",
      " player.csv\n",
      "Categorical columns: 3\n",
      "• full_name: 4791 unique values\n",
      "• first_name: 1568 unique values\n",
      "• last_name: 2904 unique values\n",
      "\n",
      " team.csv\n",
      "Categorical columns: 5\n",
      "• full_name: 30 unique values\n",
      "• abbreviation: 30 unique values\n",
      "• nickname: 30 unique values\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA CONSISTENCY ANALYSIS - Check for Uniform Data Formats\")\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    print(f\"\\n {filename}\")\n",
    "    \n",
    "    # Check object/categorical columns for consistency\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Categorical columns: {len(object_cols)}\")\n",
    "        for col in object_cols[:3]:  # Show first 3 as sample\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"• {col}: {unique_count} unique values\")\n",
    "    else:\n",
    "        print(f\"No categorical columns to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23649a",
   "metadata": {},
   "source": [
    "## Data Uniqueness Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0b631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA UNIQUENESS ANALYSIS - Identify Duplicate Rows\n",
      "\n",
      " common_player_info.csv\n",
      "  ✓ All rows are unique (4171 unique rows)\n",
      "\n",
      " game.csv\n",
      "  ✓ All rows are unique (65698 unique rows)\n",
      "\n",
      " game_summary.csv\n",
      "  ⚠ Duplicate rows found: 40 (0.07%)\n",
      "    Unique rows: 58070/58110\n",
      "\n",
      " inactive_players.csv\n",
      "  ⚠ Duplicate rows found: 7 (0.01%)\n",
      "    Unique rows: 110184/110191\n",
      "\n",
      " line_score.csv\n",
      "  ⚠ Duplicate rows found: 40 (0.07%)\n",
      "    Unique rows: 58013/58053\n",
      "\n",
      " other_stats.csv\n",
      "  ⚠ Duplicate rows found: 10 (0.04%)\n",
      "    Unique rows: 28261/28271\n",
      "\n",
      " play_by_play.csv\n",
      "  ⚠ Duplicate rows found: 40 (0.07%)\n",
      "    Unique rows: 58013/58053\n",
      "\n",
      " other_stats.csv\n",
      "  ⚠ Duplicate rows found: 10 (0.04%)\n",
      "    Unique rows: 28261/28271\n",
      "\n",
      " play_by_play.csv\n",
      "  ⚠ Duplicate rows found: 7360 (0.05%)\n",
      "    Unique rows: 13585539/13592899\n",
      "\n",
      " player.csv\n",
      "  ✓ All rows are unique (4831 unique rows)\n",
      "\n",
      " team.csv\n",
      "  ✓ All rows are unique (30 unique rows)\n",
      "  ⚠ Duplicate rows found: 7360 (0.05%)\n",
      "    Unique rows: 13585539/13592899\n",
      "\n",
      " player.csv\n",
      "  ✓ All rows are unique (4831 unique rows)\n",
      "\n",
      " team.csv\n",
      "  ✓ All rows are unique (30 unique rows)\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA UNIQUENESS ANALYSIS - Identify Duplicate Rows\")\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    print(f\"\\n {filename}\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    total_rows = len(df)\n",
    "    unique_rows = len(df.drop_duplicates())\n",
    "    duplicate_rows = total_rows - unique_rows\n",
    "    \n",
    "    if duplicate_rows > 0:\n",
    "        duplicate_pct = (duplicate_rows / total_rows) * 100\n",
    "        print(f\"  ⚠ Duplicate rows found: {duplicate_rows} ({duplicate_pct:.2f}%)\")\n",
    "        print(f\"    Unique rows: {unique_rows}/{total_rows}\")\n",
    "    else:\n",
    "        print(f\"  ✓ All rows are unique ({total_rows} unique rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff543c93",
   "metadata": {},
   "source": [
    "## Quality Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1807d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE DATA QUALITY SUMMARY REPORT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/l5wmp_6j6lv_1lmyqfc1vhww0000gn/T/ipykernel_67920/195226449.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  col_dt = pd.to_datetime(df[dt_cols[0]], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           Dataset     Rows  Cols Completeness % Uniqueness % Consistency               Timeliness Accuracy % Outliers % Validity\n",
      "common_player_info     4171    33          96.74       100.00      0.2696 1900-01-01 to 2003-11-18      64.06      35.94   94.39%\n",
      "              game    65698    55          88.02       100.00      0.0272 1946-11-01 to 2023-06-12      48.00      52.00   81.40%\n",
      "      game_summary    58110    14          80.39        99.93      0.2029 1946-11-01 to 2023-06-12      85.71      14.29   87.45%\n",
      "  inactive_players   110191     9         100.00        99.99      0.0048                      N/A      92.54       7.46  100.00%\n",
      "        line_score    58053    43          68.14        99.93      0.0347 1946-11-01 to 2023-06-12      80.91      19.09   62.11%\n",
      "       other_stats    28271    26          98.79        99.96      0.0019 1970-01-01 to 1970-01-01      62.92      37.08   91.93%\n",
      "      play_by_play 13592899    34          63.84        99.95      0.0142 2025-12-07 to 2025-12-07      36.30      63.70   61.76%\n",
      "            player     4831     5          99.98       100.00      0.6391                      N/A      80.77      19.23  100.00%\n",
      "              team       30     7         100.00       100.00      0.9467                      N/A     100.00       0.00  100.00%\n",
      "✓ Report exported to: data_quality_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Data Quality Summary Report\n",
    "# Calculate all seven quality dimensions for each dataset\n",
    "\n",
    "print(\"COMPREHENSIVE DATA QUALITY SUMMARY REPORT\")\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for filename, df in sorted(dfs.items()):\n",
    "    dataset_name = filename.replace('.csv', '')\n",
    "    rows = len(df)\n",
    "    cols = len(df.columns)\n",
    "    \n",
    "    # 1. COMPLETENESS: % of non-null values\n",
    "    total_cells = rows * cols if cols else 0\n",
    "    missing_cells = int(df.isnull().sum().sum())\n",
    "    completeness = 100.0 * (1 - (missing_cells / total_cells)) if total_cells else 100.0\n",
    "    \n",
    "    # 2. UNIQUENESS: % of duplicate rows\n",
    "    dup_count = int(df.duplicated().sum())\n",
    "    uniqueness = 100.0 * (1 - (dup_count / rows)) if rows else 100.0\n",
    "    \n",
    "    # 3. CONSISTENCY: avg unique count ratio for categorical columns\n",
    "    obj_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    ratios = []\n",
    "    for col in obj_cols:\n",
    "        if rows:\n",
    "            ratios.append(df[col].nunique(dropna=True) / rows)\n",
    "    consistency = float(np.mean(ratios)) if ratios else 1.0\n",
    "    \n",
    "    # 4. TIMELINESS: date range for datetime columns (or \"N/A\")\n",
    "    dt_cols = [c for c, t in df.dtypes.items() if np.issubdtype(t, np.datetime64)]\n",
    "    if not dt_cols:\n",
    "        guess = [c for c in df.columns if 'date' in c.lower() or 'time' in c.lower()]\n",
    "        dt_cols = guess[:1] if guess else []\n",
    "    \n",
    "    timeliness_range = 'N/A'\n",
    "    if dt_cols:\n",
    "        try:\n",
    "            col_dt = pd.to_datetime(df[dt_cols[0]], errors='coerce')\n",
    "            min_dt = col_dt.min()\n",
    "            max_dt = col_dt.max()\n",
    "            if not pd.isna(min_dt) and not pd.isna(max_dt):\n",
    "                timeliness_range = f\"{min_dt.date().isoformat()} to {max_dt.date().isoformat()}\"\n",
    "        except Exception:\n",
    "            timeliness_range = 'N/A'\n",
    "    \n",
    "    # 5. ACCURACY: % of rows with outliers (IQR method on numeric columns)\n",
    "    numeric = df.select_dtypes(include=[np.number])\n",
    "    outlier_rows = set()\n",
    "    for col in numeric.columns:\n",
    "        s = numeric[col].dropna()\n",
    "        if s.empty:\n",
    "            continue\n",
    "        Q1 = s.quantile(0.25)\n",
    "        Q3 = s.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        mask = (df[col] < lower) | (df[col] > upper)\n",
    "        outlier_rows.update(df[mask].index.tolist())\n",
    "    \n",
    "    accuracy = 100.0 * (1 - (len(outlier_rows) / rows)) if rows else 100.0\n",
    "    outlier_pct = 100.0 * (len(outlier_rows) / rows) if rows else 0.0\n",
    "    \n",
    "    # 6. VALIDITY: Check for realistic NBA data types and ranges\n",
    "    validity_checks = []\n",
    "    \n",
    "    # Check IDs (player_id, game_id, team_id) - should be positive numeric\n",
    "    id_cols = [c for c in df.columns if 'id' in c.lower() and df[c].dtype in ['int64', 'float64']]\n",
    "    for col in id_cols:\n",
    "        # IDs should be positive and non-zero\n",
    "        valid_ids = ((df[col] > 0) & (df[col].notna())).sum()\n",
    "        if len(df[col]) > 0:\n",
    "            validity_checks.append(100.0 * valid_ids / len(df[col]))\n",
    "    \n",
    "    # Check percentage columns - should be 0-100\n",
    "    pct_cols = [c for c in df.columns if 'pct' in c.lower() or 'percentage' in c.lower()]\n",
    "    for col in pct_cols:\n",
    "        if col in df.columns and df[col].dtype in ['int64', 'float64']:\n",
    "            valid_pcts = ((df[col] >= 0) & (df[col] <= 100) & (df[col].notna())).sum()\n",
    "            if len(df[col]) > 0:\n",
    "                validity_checks.append(100.0 * valid_pcts / len(df[col]))\n",
    "    \n",
    "    # Check team abbreviations - should be 2-3 chars\n",
    "    abbr_cols = [c for c in df.columns if 'abbreviation' in c.lower() or 'abbr' in c.lower()]\n",
    "    for col in abbr_cols:\n",
    "        if col in df.columns:\n",
    "            valid_abbr = df[col].dropna().apply(lambda x: 2 <= len(str(x)) <= 3).sum()\n",
    "            if df[col].notna().sum() > 0:\n",
    "                validity_checks.append(100.0 * valid_abbr / df[col].notna().sum())\n",
    "    \n",
    "    # Check score/point columns - should be non-negative\n",
    "    score_cols = [c for c in df.columns if any(x in c.lower() for x in ['pts', 'points', 'score', 'fga', 'fgm', 'fg3a', 'fg3m', 'fta', 'ftm', 'reb', 'ast', 'stl', 'blk', 'tov', 'pf'])]\n",
    "    for col in score_cols:\n",
    "        if col in df.columns and df[col].dtype in ['int64', 'float64']:\n",
    "            valid_scores = ((df[col] >= 0) & (df[col].notna())).sum()\n",
    "            if len(df[col]) > 0:\n",
    "                validity_checks.append(100.0 * valid_scores / len(df[col]))\n",
    "    \n",
    "    # Calculate overall validity score\n",
    "    if validity_checks:\n",
    "        validity = f\"{np.mean(validity_checks):.2f}%\"\n",
    "    else:\n",
    "        validity = 'N/A'\n",
    "    \n",
    "    # Append row\n",
    "    summary_data.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Rows': rows,\n",
    "        'Cols': cols,\n",
    "        'Completeness %': f\"{completeness:.2f}\",\n",
    "        'Uniqueness %': f\"{uniqueness:.2f}\",\n",
    "        'Consistency': f\"{consistency:.4f}\",\n",
    "        'Timeliness': timeliness_range,\n",
    "        'Accuracy %': f\"{accuracy:.2f}\",\n",
    "        'Outliers %': f\"{outlier_pct:.2f}\",\n",
    "        'Validity': validity\n",
    "    })\n",
    "\n",
    "# Create and display summary table\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Export to CSV\n",
    "summary_df.to_csv('data_quality_report.csv', index=False)\n",
    "print(\"✓ Report exported to: data_quality_report.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
